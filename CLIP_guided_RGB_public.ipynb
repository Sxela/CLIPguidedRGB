{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP guided RGB public.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo6n2TFkT_gV"
      },
      "source": [
        "# CLIP is all you need (c) [crumb](https://twitter.com/aicrumb)\n",
        "Greatly inspired by [this tweet](https://twitter.com/aicrumb/status/1448351059957764096/photo/1) and all the CLIP guided approaches.\n",
        "\n",
        "What if we directly optimize the raw image tensor using CLIP instead of tuning a generator network or its inputs? \n",
        "Just like style transfer algos were doing 5 years ago :D\n",
        "\n",
        "by [sxela](https://github.com/Sxela)\n",
        "\n",
        "this notebook's repo: [github](https://github.com/Sxela/CLIPguidedRGB)\n",
        "\n",
        "tip me: [paypal](http://paypal.me/sx3la)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-YIOW6SULgr7"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson, Alexander Spirin\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YM7FT6lKmOB"
      },
      "source": [
        "# Vanilla CLIP guided RBG\n",
        "Slow but fancy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kjItp8VT5dv"
      },
      "source": [
        "#installation. run once\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP -qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxJyCx3HUw21"
      },
      "source": [
        "#imports. run once or after restart\n",
        "%cd CLIP\n",
        "import torch\n",
        "from torchvision.transforms import *\n",
        "import clip\n",
        "import PIL\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import trange\n",
        "device='cuda'\n",
        "model = clip.load('ViT-B/32',jit=False)[0].eval().requires_grad_(False).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOvlCjC0UzCW"
      },
      "source": [
        "#define functions. run once or after restart\n",
        "def get_sizes(sz, min_sz=32):\n",
        "  szs = [sz]\n",
        "  while True:\n",
        "    if sz<=min_sz: return sorted(szs)\n",
        "    if sz%2==0:\n",
        "      sz = sz//2\n",
        "      szs.append(sz)\n",
        "    else: return sorted(szs)\n",
        "  return sorted(szs)\n",
        "\n",
        "def make_crop(img, ratio, max_cut=224, min_cut=0.2):\n",
        "  w, h = img.shape[2:]\n",
        "  min_sz = min(w,h)\n",
        "  if min_cut<1: min_cut = int(min_sz*min_cut)\n",
        "  crop_size = int(min(max(ratio*min_sz, min_cut), max_cut, min_sz))\n",
        "\n",
        "  w_offset = int(torch.rand(1)*(w-crop_size))\n",
        "  h_offset = int(torch.rand(1)*(h-crop_size))\n",
        "\n",
        "  cropped = img[:,:,w_offset:w_offset+crop_size,h_offset:h_offset+crop_size]\n",
        "  return f(cropped)\n",
        "\n",
        "def get_crops(img, ratios, max_cut, min_cut):\n",
        "  return torch.cat([make_crop(img, ratio.item(), max_cut, min_cut) for ratio in ratios])\n",
        "\n",
        "def show_img(t):\n",
        "    img = PIL.Image.fromarray((t.permute(0,2,3,1)*127.5+128).clamp(0,255).to(torch.uint8)[0].cpu().numpy(),'RGB')\n",
        "    display(img)\n",
        "\n",
        "def range_loss(input):\n",
        "    #taken from this colab https://colab.research.google.com/drive/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA#scrollTo=YHOj78Yvx8jP\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])  \n",
        "\n",
        "def tv_loss(input):\n",
        "    #taken from this colab https://colab.research.google.com/drive/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA#scrollTo=YHOj78Yvx8jP\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "def fit(t, size, steps=1000, ncut=8, max_sz=224, min_sz=32, use_weighted_ratios=True):\n",
        "  z2 = F.interpolate(t, (size,size), mode='bicubic')\n",
        "  t = z2.detach().clone().requires_grad_(True)\n",
        "  show_img(t)\n",
        "  opt=torch.optim.Adam([t],lr=lr)\n",
        "  for i in trange(steps):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    ratios = [torch.ones(1).cuda()]\n",
        "    for j in range(ncut):\n",
        "      ratios.append(torch.rand(1).cuda())\n",
        "    ratios = torch.cat(ratios)\n",
        "    crops = get_crops(t, ratios, max_sz, min_sz)\n",
        "    loss_avg = 0.\n",
        "    loss = 0.\n",
        "    weighted_ratios = ratios/ratios.sum() if use_weighted_ratios else torch.ones_like(ratios).to(device)\n",
        "\n",
        "    embeds = model.encode_image(crops)\n",
        "\n",
        "    for embed, ratio in zip(embeds, weighted_ratios):\n",
        "      x = F.normalize(embed, dim=-1)\n",
        "      loss+=torch.sqrt(criterion(x, y))*ratio\n",
        "\n",
        "    for crop, ratio in zip(crops, weighted_ratios):\n",
        "      loss+=range_loss(crop[None,...]).sum()*range_loss_w\n",
        "      loss+=tv_loss(crop[None,...]).sum()*tv_loss_w\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loss_avg = loss if loss_avg==0. else (loss_avg*loss_lerp+loss*(1-loss_lerp))\n",
        "    if i % 100 == 0: \n",
        "      print(loss_avg.item())\n",
        "    if i % 500 == 0: \n",
        "      show_img(t)\n",
        "  show_img(t)\n",
        "  return t\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "f=Compose([Resize(224),\n",
        "          Lambda(lambda x:torch.clamp((x+1)/2,0,1)),\n",
        "          RandomGrayscale(p=.2),\n",
        "          Lambda(lambda x: x+torch.randn_like(x)*0.01)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCY-IRXuU5uc"
      },
      "source": [
        "#set parameters and train\n",
        "prompt = 'a landscape containing knights riding on the horizon by Greg Rutkowski' #text prompt\n",
        "seed = 0 \n",
        "torch.manual_seed(seed)\n",
        "\n",
        "tv_loss_w = 0 #increase to reduce image noise. 0.01 is a good start\n",
        "range_loss_w = 0 #increase to reduce image burn. 150 is a good start \n",
        "\n",
        "szs = get_sizes(1024, 64); print(szs) #getting sizes\n",
        "steps = [2000]*len(szs) #getting number of steps per size\n",
        "cuts = [8,8,8,16,24] #number of image cuts for CLIP loss per image size\n",
        "\n",
        "#max_sz 64 and min_sz 0.2 produce highly detailed crisp abstract patterns with lots of objects\n",
        "#max_sz 224 and min_sz 48 produce blurry image with fewer objects (you can experiment with those)\n",
        "max_szs=[64]*len(szs) #max cut size (pixels)\n",
        "min_szs=[0.2]*len(szs) #min cut size (pixel or image size ratio)\n",
        "\n",
        "lr=1e-2\n",
        "loss_lerp = 0.6 #used for display only\n",
        "\n",
        "encoded_prompt = model.encode_text(clip.tokenize(prompt).to(device))\n",
        "y = F.normalize(encoded_prompt, dim=-1)\n",
        "\n",
        "#init image, can be replaced with a photo\n",
        "z=torch.rand((1,3,szs[0],szs[0]),device=device,requires_grad=True)\n",
        "\n",
        "for size, step, cut, max_sz, min_sz in zip(szs, steps, cuts, max_szs, min_szs):\n",
        "  print(size, step, cut)\n",
        "  z = fit(z, size, steps=step, ncut=cut, max_sz=max_sz, min_sz=min_sz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMIY5-l3BI2C"
      },
      "source": [
        "# CLIP guided Point Cloud (experimental)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd7SMDrHWYyf"
      },
      "source": [
        "#installation. run once\n",
        "#taken from https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/render_colored_points.ipynb\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP -qq\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"1.9\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{torch.__version__[0:5:2]}\"\n",
        "        ])\n",
        "        !pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
        "        !tar xzf 1.10.0.tar.gz\n",
        "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65CIo-umBT9m"
      },
      "source": [
        "#imports. run once or after restart\n",
        "%cd CLIP\n",
        "import torch\n",
        "from torchvision.transforms import *\n",
        "import clip\n",
        "import PIL\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import trange\n",
        "device='cuda'\n",
        "model = clip.load('ViT-B/32',jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "#taken from https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/render_colored_points.ipynb\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Util function for loading point clouds|\n",
        "import numpy as np\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Pointclouds\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    FoVOrthographicCameras, \n",
        "    PointsRasterizationSettings,\n",
        "    PointsRenderer,\n",
        "    PulsarPointsRenderer,\n",
        "    PointsRasterizer,\n",
        "    AlphaCompositor,\n",
        "    NormWeightedCompositor\n",
        ")\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74B5LIRdBXOi"
      },
      "source": [
        "#define functions. run once or after restart\n",
        "def get_sizes(sz, min_sz=32):\n",
        "  szs = [sz]\n",
        "  while True:\n",
        "    if sz<=min_sz: return sorted(szs)\n",
        "    if sz%2==0:\n",
        "      sz = sz//2\n",
        "      szs.append(sz)\n",
        "    else: return sorted(szs)\n",
        "  return sorted(szs)\n",
        "\n",
        "def make_crop(img, ratio, max_cut=224, min_cut=0.2):\n",
        "  w, h = img.shape[2:]\n",
        "  min_sz = min(w,h)\n",
        "  if min_cut<1: min_cut = int(min_sz*min_cut)\n",
        "  crop_size = int(min(max(ratio*min_sz, min_cut), max_cut, min_sz))\n",
        "\n",
        "  w_offset = int(torch.rand(1)*(w-crop_size))\n",
        "  h_offset = int(torch.rand(1)*(h-crop_size))\n",
        "\n",
        "  cropped = img[:,:,w_offset:w_offset+crop_size,h_offset:h_offset+crop_size]\n",
        "  return f(cropped)\n",
        "\n",
        "def get_crops(img, ratios, max_cut, min_cut):\n",
        "  return torch.cat([make_crop(img, ratio.item(), max_cut, min_cut) for ratio in ratios])\n",
        "\n",
        "def show_img(vertex, rgb, renderer):\n",
        "    img = render(vertex, rgb, renderer)\n",
        "    display(PIL.Image.fromarray((img[0]*255).clamp(0,255).detach().cpu().numpy().astype('uint8'),'RGB'))\n",
        "\n",
        "def save_render(render, path):\n",
        "    PIL.Image.fromarray((render[0]*255).clamp(0,255).detach().cpu().numpy().astype('uint8'),'RGB').save(path)\n",
        "\n",
        "def range_loss(input):\n",
        "    #taken from this colab https://colab.research.google.com/drive/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA#scrollTo=YHOj78Yvx8jP\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])  \n",
        "\n",
        "def tv_loss(input):\n",
        "    #taken from this colab https://colab.research.google.com/drive/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA#scrollTo=YHOj78Yvx8jP\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "def get_img_loss(img, ncut=8, max_sz=224, min_sz=32, use_weighted_ratios=True):\n",
        "    ratios = [torch.ones(1).cuda()]\n",
        "    img = img.permute(0,3,1,2)\n",
        "    for j in range(ncut):\n",
        "      ratios.append(torch.rand(1).cuda())\n",
        "    ratios = torch.cat(ratios)\n",
        "    crops = get_crops(img, ratios, max_sz, min_sz)\n",
        "\n",
        "    loss = 0.\n",
        "    weighted_ratios = ratios/ratios.sum() if use_weighted_ratios else torch.ones_like(ratios).to(device)\n",
        "    embeds = model.encode_image(crops)\n",
        "\n",
        "    for embed, ratio in zip(embeds, weighted_ratios):\n",
        "      x = F.normalize(embed, dim=-1)\n",
        "      loss+=torch.sqrt(criterion(x, y))*ratio\n",
        "\n",
        "    for crop, ratio in zip(crops, weighted_ratios):\n",
        "      loss+=range_loss(crop[None,...]).sum()*range_loss_w\n",
        "      loss+=tv_loss(crop[None,...]).sum()*tv_loss_w\n",
        "        \n",
        "    return loss\n",
        "\n",
        "def render(var_h, rgb, renderer):\n",
        "  p = Pointclouds(points=[var_h], features=[rgb])\n",
        "  return renderer(p)[...,:3]\n",
        "\n",
        "def fit(var_h, rgb, steps=1000, ncut=8, max_sz=224, min_sz=32, use_weighted_ratios=True, renderer=None):\n",
        "\n",
        "  show_img(var_h, rgb, renderer)\n",
        "  opt=torch.optim.Adam([{'params': rgb,'lr': lr}, {'params': var_h,'lr': lr_h}])\n",
        "  loss_avg = 0.\n",
        "  for i in trange(steps):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    if (rotation_step!=0) & (i % rotation_step == 0):\n",
        "      #rotate the camera every rotation_step steps\n",
        "      R, T = look_at_view_transform(20, 10, (i//rotation_step)%359)\n",
        "      cameras = FoVOrthographicCameras(device=device, R=R, T=T, znear=0.01)\n",
        "      renderer = PointsRenderer(\n",
        "        rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "        compositor=NormWeightedCompositor(background_color=(0,0,0)))\n",
        "\n",
        "    img = render(var_h, rgb, renderer)\n",
        "    loss = 0.\n",
        "    loss+=get_img_loss(img, ncut=ncut, max_sz=max_sz, min_sz=min_sz, use_weighted_ratios=use_weighted_ratios)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    loss_avg = loss if loss_avg==0. else (loss_avg*loss_lerp+loss*(1-loss_lerp))\n",
        "    \n",
        "    if i % 100 == 0: \n",
        "      print('loss_avg', loss_avg.item())\n",
        "    if i % 300 == 0: \n",
        "      show_img(var_h, rgb, renderer)\n",
        "    if i % rotation_step == 0:\n",
        "      save_render(img, f'/content/out/{i//rotation_step:05d}.jpg')\n",
        "      \n",
        "  show_img(var_h, rgb, renderer)\n",
        "  return var_h, rgb\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "f=Compose([Resize(224),\n",
        "          Lambda(lambda x:torch.clamp(x,0,1)),\n",
        "          RandomGrayscale(p=.2),\n",
        "          Lambda(lambda x: x+torch.randn_like(x)*0.01)])\n",
        "\n",
        "loss_lerp = 0.6 #used for display only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUse1f3BY6A"
      },
      "source": [
        "!mkdir /content/out\n",
        "!rm -rf /content/out/*\n",
        "\n",
        "#set parameters and train\n",
        "prompt = '8bit pokemon #pixelart' #text prompt\n",
        "rotation_step = 100 #rotate the camera for 1 degree every N steps. Set to 0 for no rotation.\n",
        "size = 256 #image size\n",
        "step = 2000 #number of steps\n",
        "\n",
        "seed = 0 \n",
        "torch.manual_seed(seed)\n",
        "\n",
        "tv_loss_w = 0.002 #increase to reduce image noise. 0.01 is a good start\n",
        "range_loss_w = 150 #increase to reduce image burn. 150 is a good start \n",
        "\n",
        "lr = 1e-2 \n",
        "lr_h = 1e-2\n",
        "\n",
        "#max_sz 64 and min_sz 0.2 produce highly detailed crisp abstract patterns with lots of objects\n",
        "#max_sz 224 and min_sz 48 produce blurry image with fewer objects (you can experiment with those)\n",
        "max_sz=224 #max cut size (pixels)\n",
        "min_sz=64 #min cut size (pixel or image size ratio)\n",
        "cut = 8 #number of image cuts for CLIP loss\n",
        "\n",
        "sz = 64 #point cloud density\n",
        "var_h = torch.randn((sz*sz*sz,3)).cuda().div(3.).requires_grad_(True)\n",
        "rgb = torch.randn((var_h.shape[0],4)).cuda().requires_grad_(True)\n",
        "\n",
        "encoded_prompt = model.encode_text(clip.tokenize(prompt).to(device))\n",
        "y = F.normalize(encoded_prompt, dim=-1)\n",
        "\n",
        "#set up carema angle\n",
        "R, T = look_at_view_transform(20, 10, 0)\n",
        "cameras = FoVOrthographicCameras(device=device, R=R, T=T, znear=0.01)\n",
        "#set up camera image size\n",
        "raster_settings = PointsRasterizationSettings(\n",
        "      image_size=size, \n",
        "      radius = 0.003,\n",
        "      points_per_pixel = 1\n",
        "  )\n",
        "  #set up camera\n",
        "renderer = PointsRenderer(\n",
        "      rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "      compositor=NormWeightedCompositor(background_color=(0,0,0)))\n",
        "\n",
        "var_h, rgb = fit(var_h, rgb, steps=step, ncut=cut, max_sz=max_sz, min_sz=min_sz, renderer=renderer)\n",
        "\n",
        "video_name = f\"/content/video-{prompt.replace(' ','_')}_size{size}_maxsz{max_sz}.mp4\"\n",
        "!ffmpeg -pattern_type glob -i '/content/out/*.jpg' {video_name}\n",
        "files.download(video_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}