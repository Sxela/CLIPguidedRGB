{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo6n2TFkT_gV"
      },
      "source": [
        "# CLIP is all you need (c) [crumb](https://twitter.com/aicrumb)\n",
        "Greatly inspired by [this tweet](https://twitter.com/aicrumb/status/1448351059957764096/photo/1) and all the CLIP guided approaches.\n",
        "\n",
        "What if we directly optimize the raw image tensor using CLIP, instead of tuning a generator network or its inputs? \n",
        "Just like all the style transfer algos were doing 5 years ago :D\n",
        "\n",
        "by [sxela](https://github.com/Sxela)\n",
        "\n",
        "this notebook's repo: [github](https://github.com/Sxela/CLIPguidedRGB)\n",
        "\n",
        "tip me: [paypal](http://paypal.me/sx3la)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YM7FT6lKmOB"
      },
      "source": [
        "# Vanilla CLIP guided RBG\n",
        "Slow but fancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kjItp8VT5dv"
      },
      "outputs": [],
      "source": [
        "#installation. run once\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxJyCx3HUw21"
      },
      "outputs": [],
      "source": [
        "#imports. run once or after restart\n",
        "%cd CLIP\n",
        "import torch\n",
        "from torchvision.transforms import *\n",
        "import clip\n",
        "import PIL\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import trange\n",
        "device='cuda'\n",
        "model = clip.load('ViT-B/32',jit=False)[0].eval().requires_grad_(False).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOvlCjC0UzCW"
      },
      "outputs": [],
      "source": [
        "#define functions. run once or after restart\n",
        "def get_sizes(sz, min_sz=32):\n",
        "  szs = [sz]\n",
        "  while True:\n",
        "    if sz<=min_sz: return sorted(szs)\n",
        "    if sz%2==0:\n",
        "      sz = sz//2\n",
        "      szs.append(sz)\n",
        "    else: return sorted(szs)\n",
        "  return sorted(szs)\n",
        "\n",
        "def make_crop(img, ratio, max_cut=224, min_cut=0.2):\n",
        "  w, h = img.shape[2:]\n",
        "  min_sz = min(w,h)\n",
        "  if min_cut<1: min_cut = int(min_sz*min_cut)\n",
        "  crop_size = int(min(max(ratio*min_sz, min_cut), max_cut))\n",
        "\n",
        "  w_offset = int(torch.rand(1)*(w-crop_size))\n",
        "  h_offset = int(torch.rand(1)*(h-crop_size))\n",
        "\n",
        "  cropped = img[:,:,w_offset:w_offset+crop_size,h_offset:h_offset+crop_size]\n",
        "  return f(cropped)\n",
        "\n",
        "def get_crops(img, ratios, max_cut, min_cut):\n",
        "  return torch.cat([make_crop(img, ratio.item(), max_cut, min_cut) for ratio in ratios])\n",
        "\n",
        "def show_img(t):\n",
        "    img = PIL.Image.fromarray((t.permute(0,2,3,1)*127.5+128).clamp(0,255).to(torch.uint8)[0].cpu().numpy(),'RGB')\n",
        "    display(img)\n",
        "\n",
        "def fit(t, size, steps=1000, ncut=8, max_sz=224, min_sz=32, use_weighted_ratios=True):\n",
        "  z2 = F.interpolate(t, (size,size), mode='bicubic')\n",
        "  t = z2.detach().clone().requires_grad_(True)\n",
        "  show_img(t)\n",
        "  opt=torch.optim.Adam([t],lr=lr)\n",
        "  for i in trange(steps):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    ratios = [torch.ones(1).cuda()]\n",
        "    for j in range(ncut):\n",
        "      ratios.append(torch.rand(1).cuda())\n",
        "    ratios = torch.cat(ratios)\n",
        "    crops = get_crops(t, ratios, max_sz, min_sz)\n",
        "    loss_avg = 0.\n",
        "    loss = 0.\n",
        "    weighted_ratios = ratios/ratios.sum() if use_weighted_ratios else torch.ones_like(ratios).to(device)\n",
        "\n",
        "    embeds = model.encode_image(crops)\n",
        "\n",
        "    for embed, ratio in zip(embeds, weighted_ratios):\n",
        "      x = F.normalize(embed, dim=-1)\n",
        "      loss+=torch.sqrt(criterion(x, y))*ratio\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loss_avg = loss if loss_avg==0. else (loss_avg*loss_lerp+loss*(1-loss_lerp))\n",
        "    if i % 100 == 0: \n",
        "      print(loss_avg.item())\n",
        "    if i % 500 == 0: \n",
        "      show_img(t)\n",
        "  show_img(t)\n",
        "  return t\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "f=Compose([Resize(224),\n",
        "          Lambda(lambda x:torch.clamp((x+1)/2,0,1)),\n",
        "          RandomGrayscale(p=.2),\n",
        "          Lambda(lambda x: x+torch.randn_like(x)*0.01)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCY-IRXuU5uc"
      },
      "outputs": [],
      "source": [
        "#set parameters and train\n",
        "prompt = 'a landscape containing knights riding on the horizon by Greg Rutkowski' #text prompt\n",
        "seed = 0 \n",
        "torch.manual_seed(seed)\n",
        "\n",
        "szs = get_sizes(1024, 64); print(szs) #getting sizes\n",
        "steps = [2000]*len(szs) #getting number of steps per size\n",
        "cuts = [8,8,8,16,24] #number of image cuts for CLIP loss per iteration\n",
        "max_szs=[64]*len(szs) #max cut size (pixels)\n",
        "min_szs=[0.2]*len(szs) #min cut size (pixel or image size ratio)\n",
        "\n",
        "lr=1e-2\n",
        "loss_lerp = 0.6 #used for display only\n",
        "\n",
        "encoded_prompt = model.encode_text(clip.tokenize(prompt).to(device))\n",
        "y = F.normalize(encoded_prompt, dim=-1)\n",
        "\n",
        "#init image, can be replaced with a photo\n",
        "z=torch.rand((1,3,szs[0],szs[0]),device=device,requires_grad=True)\n",
        "\n",
        "for size, step, cut, max_sz, min_sz in zip(szs, steps, cuts, max_szs, min_szs):\n",
        "  print(size, step, cut)\n",
        "  z = fit(z, size, steps=step, ncut=cut, max_sz=max_sz, min_sz=min_sz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd7SMDrHWYyf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CLIP guided RGB public.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
